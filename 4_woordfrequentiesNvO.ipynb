{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Woordfrequenties - hoe vaak komt een woord voor in een document of een corpus?\n",
    "\n",
    "Welke woorden komen het meeste voor in een bepaald egodocument? De onderstaande code berekent de frequenties van alle woorden in een enkele tekst. De tekst waarin wordt gezocht wordt bepaald door de waarde van de variabele `egodocument`. De code toont vervolgens de 30 meest voorkomende woorden in deze tekst. De variabele `max` het aantal woorden dat wordt getoond. \n",
    "\n",
    "Ook hier is het van belang de stopwoordenlijst te gebruiken om alleen zinvolle frequenties in de resultaten terug te krijgen die te maken hebben met de inhoud, en om niet ter zake doende lidwoorden, bijwoorden en voorzetsels er uit te filteren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kitlvTdm import *\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = ['word','frequentie', 'total_tokens'])\n",
    "\n",
    "max = 150 # Het maximum aantal woorden dat later wordt geprint en opgeslagen\n",
    "path = 'corpus' # Het pad naar de map met de corpus\n",
    "outfile = \"corpus_frequency\"\n",
    "\n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x], reverse = True)\n",
    "\n",
    "# de totale frequenties worden opgeslagen in een dictionary\n",
    "freqTotal = dict() \n",
    "\n",
    "\n",
    "\n",
    "# Hier worden het aantal woorden (tokens) en documenten bijgehouden\n",
    "tokensTotal = 0\n",
    "egodocuments = 0\n",
    "\n",
    "for i, file in enumerate(os.listdir(path)):\n",
    "    \n",
    "    percentage_done = i / len(os.listdir(path)) * 100\n",
    "    print(\"percentage done: {:.2f}%\".format(percentage_done), end = \"\\r\")\n",
    "    \n",
    "    if re.search( 'txt$' , file ):\n",
    "        egodocuments += 1\n",
    "        \n",
    "        # bereken per woord hoe vaak het voor komt\n",
    "        freq = calculateWordFrequencies( join( path, file ) )\n",
    "        \n",
    "        # Haal de stopwoorden eruit\n",
    "        freq = removeStopwords(freq)\n",
    "\n",
    "        for word in freq:\n",
    "            if word  in freqTotal: # Als het woord al in freqTotal staat: tel extra aantal erbij op\n",
    "                freqTotal[word] += freq[word]\n",
    "            else: # anders, plaats het woord in freqTotal\n",
    "                freqTotal[word] = freq[word]\n",
    "        \n",
    "        tokensTotal += numberOfTokens( join( path, file ) )\n",
    "\n",
    "        if i > 50:\n",
    "            break\n",
    "            \n",
    "for count, word in enumerate(sortedByValue(freqTotal)):\n",
    "    print( f' { word } => { freqTotal[word] / tokensTotal }' )\n",
    "    \n",
    "    df = df.append({'word' : word, 'frequency' : freqTotal[word] / tokensTotal, 'total_tokens': tokensTotal}, ignore_index = True)\n",
    "    \n",
    "    if count == max:\n",
    "        break\n",
    "        \n",
    "df.to_excel(f\"{outfile}.xlsx\")\n",
    "print(f\"Excel file saved as {outfile}.xlsx\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 4: Bepaal de meest frequente woorden in een van de egodocumenten in het corpus van \"Soldaat in Indonesië\". Experimenteer met verschillende waarden voor de variabelen `egodocument` en `max`.**\n",
    "\n",
    "De woordfrequenties geven deels een vertekend beeld, omdat in veel memoires letterlijk passages of hele delen van dagboeken uit de tijd zelf worden gebruikt. Eigenlijk zou je die boeken die verhoudingsgewijze veel van deze teksten bevatten moeten kunnen isoleren, zodat de boeken zonder deze passages kunnen worden geanalyseerd. Dat kan alleen door extra codes aan de publicaties toe te voegen die dit verschil aangeeft. Die laag is er nu nog niet, maar staat wel in de planning.\n",
    "\n",
    "Veranderingen in woordgebruik zijn gebonden aan tijd en veranderingen in de samenleving. Omdat dit corpus zich uitstrekt vanaf de periode van het conflict zelf (1945-1949) tot aan 2017, is het interessant om het voorkomen van bepaalde termen chronologisch te vergelijken. De onderstaande code verdeelt het corpus in perioden van 5 jaar, en berekent vervolgens de woordfrequenties voor de egodocumenten die in deze verschillende tijdvakken verschenen. Hierbij moet wel de kanttekening worden geplaatst dat niet alle egodocumenten konden worden gedateerd. Bij deze analyse worden de teksten die nog niet zijn gedateerd genegeerd. Verder is het uiteraard ook zo dat er een onevenredige verdeling is van het aantal boeken over deze perioden. De absolute frequenties kunnen daardoor niet zonder meer worden vergeleken. OM de frequenties toch vergelijkbaar te maken zijn de absolute tellingen steeds gedeeld door het totaal aantal woorden in de egoducmenten uit de verschillende perioden.\n",
    "\n",
    "De lengte van de geanlyseerde periode kan overgens worden aangepast via de variabele `period_length`. \n",
    "\n",
    "De resultaten worden getoond in dit Notebook, maar worden eveneens weggeschreven in een bestand met de naam `frequency_chronological.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitlvTdm import *\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# specificeer hier de start en eind datum om te doorzoeken\n",
    "start = 1930\n",
    "end = 2014\n",
    "\n",
    "# specificeer hier de lengte van de perioden en het aantal woorden voor printen/opslaan\n",
    "period_length = 5\n",
    "max = 150\n",
    "\n",
    "path = 'corpus'\n",
    "outfile = f\"woordfrequenties_perioden_{start}_{end}_{period_length}\"\n",
    "\n",
    "df = pd.DataFrame(columns = [\"period\", \"word\", \"frequency\", \"total_tokens_period\"])\n",
    "\n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "\n",
    "# Hier worden de verschillende tijdsperioden berekent\n",
    "intervals = []\n",
    "for year in range( start , end , period_length ):\n",
    "    intervals.append(year)\n",
    "    \n",
    "\n",
    "for year in intervals:\n",
    "    year_from = year\n",
    "    year_to =  period_length - 1 + year\n",
    "    freqTotal = dict()\n",
    "    \n",
    "    tokensTotal = 0\n",
    "    egodocuments = 0\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if re.search( 'txt$' , file ):\n",
    "            year = showYear( file )\n",
    "            if year != \"\" and len(year) == 4:\n",
    "                year = int(year)\n",
    "                if year >= year_from and year <= year_to:\n",
    "                    egodocuments += 1\n",
    "                    freq = calculateWordFrequencies( join( path, file ) )\n",
    "                    \n",
    "                    for word in freq:\n",
    "                        if word  in freqTotal: # Als het woord al in freqTotal staat: tel extra aantal erbij op\n",
    "                            freqTotal[word] += freq[word]\n",
    "                        else: # anders, plaats het woord in freqTotal\n",
    "                            freqTotal[word] = freq[word]\n",
    "                    \n",
    "                    tokensTotal += numberOfTokens( join( path, file ) )\n",
    "\n",
    "    print( f'\\n{ year_from }-{ year_to }\\n{tokensTotal} words in total in {egodocuments} egodocuments\\n\\n' )\n",
    "\n",
    "\n",
    "    freqTotal = removeStopwords( freqTotal )\n",
    "\n",
    "    count = 0\n",
    "    for word in reversed( sortedByValue(freqTotal) ):\n",
    "        \n",
    "        df = df.append({\"period\" : f\"{year_from}-{year_to}\", \"word\" : word,\n",
    "                        \"frequency\" : freqTotal[word] / tokensTotal, \n",
    "                        \"total_tokens_period\": tokensTotal }, ignore_index = True)\n",
    "        \n",
    "        print( f' { word } => { freqTotal[word] / tokensTotal }' )\n",
    "        count += 1\n",
    "        if count == max:\n",
    "            break\n",
    "\n",
    "df.to_excel(f\"{outfile}.xlsx\")\n",
    "print(f\"Excel file saved as {outfile}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 4: Probeer met behulp van de bovenstaande code te verkennen hoe het woordgebruik zich ontwikkelde over de loop van de afgelopen decennia. Verander hiervoor de waarde van de variabelen `period_length` en `nr_words`.**\n",
    "\n",
    "Woordsoorten als zelfstandige naamwoorden, bijvoeglijke naamwoorden en werkwoorden drukken zijn over het algemeen het meest bepalend voor de betekenis van zinnen. Het kan daarom nuttig en informatief zijn om frequentie-analyses te beperken tot dit soort woorden. In de onderstaande cellen worden uitsluitend de zelfstandige naamwoorden, bijvoeglijke naamwoorden en werkwoorden geteld, door gebruik te maken van de hierboven al genoemde module `nltk`. Deze module richt zich normaal gesproken op Engelstalige teksten. Om `nltk` ook toe te kunnen passen op Nederlandstalige teksten moet eerst de onderstaande code worden uitgevoerd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('alpino')\n",
    "\n",
    "from nltk.corpus import alpino as alp\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "training_corpus = alp.tagged_sents()\n",
    "unitagger = UnigramTagger(training_corpus)\n",
    "bitagger = BigramTagger(training_corpus, backoff=unitagger)\n",
    "pos_tag = bitagger.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als de installatie van de Nederlandstalige variant van `nltk` geen problemen opleverde, kan de onderstaande, meer gerichte frequentie-analyse worden uitgevoerd. Let er hierbij op dat het toekennen van grammaticale categorieën wel enige rekenkracht vergt. Het uitvoeren van de code kan dus enige tijd in beslag nemen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kitlvTdm import *\n",
    "import os\n",
    "from os.path import join\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nr_words = 100\n",
    "tokens_total = 0\n",
    "path = 'corpus'\n",
    "egodocument = '03391.txt'\n",
    "\n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "\n",
    "\n",
    "\n",
    "out = open( 'frequency_POS.csv' , 'w' , encoding = 'utf-8' )\n",
    "out.write( 'word,frequency\\n' )\n",
    "\n",
    "\n",
    "\n",
    "freqTotal = dict()\n",
    "\n",
    "countFile = 0 \n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if re.search( 'txt$' , file ):\n",
    "        countFile += 1\n",
    "        if file == egodocument:\n",
    "        #print( '\\rReading {} ... ({}/577)'.format(file , countFile ) )\n",
    "            with open( join( path, file ), encoding = \"utf-8\") as fileName:\n",
    "                print(fileName)\n",
    "                fullText = fileName.read()\n",
    "                sent = sent_tokenize(fullText)\n",
    "                for s in sent:\n",
    "                    words = word_tokenize(s)\n",
    "                    pos = pos_tag(words)\n",
    "                    tokens_total += len(words)\n",
    "                    for p in pos:\n",
    "                        if p[1] is not None:\n",
    "                            if re.search( r'^(adj)|(noun)|(verb)' , p[1] ):\n",
    "                                freqTotal[ p[0]  ] = freqTotal.get( p[0] ,0 ) + 1\n",
    "\n",
    "\n",
    "count = 0                                \n",
    "freqTotal = removeStopwords( freqTotal )\n",
    "for word in reversed( sortedByValue(freqTotal) ):\n",
    "    out.write( f'{word},{freqTotal[word]}\\n' )\n",
    "    print( f' { word } => { freqTotal[word] / tokens_total }' )\n",
    "    count += 1\n",
    "    if count == nr_words:\n",
    "        break\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 5: Probeer een lijst te generen van de 150 meest frequente zelfstandige naamwoorden, bijvoeglijke naamwoorden en werkwoorden.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
